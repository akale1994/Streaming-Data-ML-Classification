{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Producing the data\n",
    "## 1.1 Process Event Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import csv\n",
    "import datetime as dt\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read the csv file into a dictionary\n",
    "def read_csv(fileName):\n",
    "    data = []\n",
    "    with open(fileName, newline = '') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            data.append(row)  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to publish the final message\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        producer_instance.send(topic_name, data)\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to connect to the kafka producer\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers = ['localhost:9092'],\n",
    "                                  value_serializer = lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version = (0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the first lot of messages\n",
    "def publish1(cRows, ts):\n",
    "    # Define an end point for the data using a random number generator\n",
    "    end = random.randint(10, 50)\n",
    "    # Select the data based on the end point\n",
    "    data = cRows[:end]\n",
    "    # Add the generated timestamp in each dictionary of data\n",
    "    for item in data:\n",
    "        item.update(ts)\n",
    "    # Return the data and end point to be used as the start point for the next data\n",
    "    return data, end\n",
    "\n",
    "# Define a function to get the messages after the first set of messages have been published\n",
    "def publish2(cRows, start, ts):\n",
    "    # Define an end point for the data using a random number generator\n",
    "    end = start + random.randint(10, 50)\n",
    "    # Select the data based on the end point\n",
    "    data = cRows[start:end]\n",
    "    # Update the start point\n",
    "    start = end\n",
    "    # Add the generated timestamp in each dictionary of data\n",
    "    for item in data:\n",
    "        item.update(ts)\n",
    "    # Return the data and end point to be used as the start point for the next data\n",
    "    return data, start\n",
    "\n",
    "# Define a function to call the publish2 function created above for each machine filtered data and append it as returned\n",
    "# Also, check if there is no data returned\n",
    "def publish2_final(start, machine, data, ts):\n",
    "    machine_data, start_ret = publish2([d for d in cRows if d['machine'] == machine], start, ts)\n",
    "    if machine_data:\n",
    "        data.append(machine_data)\n",
    "    # If the data is finished, restart it from the first sequence\n",
    "    if not machine_data:\n",
    "        machine_data, start_ret = publish1([d for d in cRows if d['machine'] == machine], ts)\n",
    "        data.append(machine_data)   \n",
    "    return data, start_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'process_stream'\n",
    "    cRows = read_csv('data_and_models/Streaming_Linux_process.csv')\n",
    "    \n",
    "    print('Publishing records...')\n",
    "    producer = connect_kafka_producer()\n",
    "\n",
    "    while True:\n",
    "        # Define an empty list for the data to be published\n",
    "        data = []\n",
    "        # Generate a UTC timestamp to be added in the data\n",
    "        ts = {'ts': int(dt.datetime.now(timezone('UTC')).timestamp())}\n",
    "        \n",
    "        # Call the publish1 function created above for each machine filtered data and append it as returned  \n",
    "        data_4, start_4 = publish1([d for d in cRows if d['machine'] == '4'], ts)\n",
    "        data.append(data_4)\n",
    "        data_5, start_5 = publish1([d for d in cRows if d['machine'] == '5'], ts)\n",
    "        data.append(data_5)\n",
    "        data_6, start_6 = publish1([d for d in cRows if d['machine'] == '6'], ts)\n",
    "        data.append(data_6)\n",
    "        data_7, start_7 = publish1([d for d in cRows if d['machine'] == '7'], ts)\n",
    "        data.append(data_7)\n",
    "        data_8, start_8 = publish1([d for d in cRows if d['machine'] == '8'], ts)\n",
    "        data.append(data_8)\n",
    "        \n",
    "        # Create a flat list of all the machines' data to be published\n",
    "        data = [item for sublist in data for item in sublist]\n",
    "        # Publish the data\n",
    "        publish_message(producer, topic, data)\n",
    "        \n",
    "        # Create a delay of 5 seconds between the messages\n",
    "        sleep(5)\n",
    "        print('---------------------------------------------------------------')\n",
    "        \n",
    "        # Create another infinite loop to publish the next set of messages after the first publish\n",
    "        while True:\n",
    "            # Define an empty list for the data to be published\n",
    "            data = []\n",
    "            # Generate a UTC timestamp to be added in the data\n",
    "            ts = {'ts': int(dt.datetime.now(timezone('UTC')).timestamp())}\n",
    "            \n",
    "            # Publish the further messages for each machine\n",
    "            data, start_4 = publish2_final(start_4, '4', data, ts)\n",
    "            data, start_5 = publish2_final(start_5, '5', data, ts)\n",
    "            data, start_6 = publish2_final(start_6, '6', data, ts)\n",
    "            data, start_7 = publish2_final(start_7, '7', data, ts)\n",
    "            data, start_8 = publish2_final(start_8, '8', data, ts)\n",
    "            \n",
    "            # Create a flat list of all the machines' data to be published\n",
    "            data = [item for sublist in data for item in sublist]\n",
    "            # Publish the data\n",
    "            publish_message(producer, topic, data)\n",
    "                \n",
    "            sleep(5)\n",
    "            print('---------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}